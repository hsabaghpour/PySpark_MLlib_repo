{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPrrWR39+Z5ecssHFnyzj/T",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hsabaghpour/PySpark_MLlib_repo/blob/main/PySpark_MLlib_Basic_Statistics.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.appName('corr').getOrCreate()"
      ],
      "metadata": {
        "id": "vwPXda-Hc5H3"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "04Rpi8sxczxD"
      },
      "outputs": [],
      "source": [
        "from pyspark.ml.linalg import Vectors\n",
        "from pyspark.ml.stat import Correlation\n",
        "\n",
        "data = [(Vectors.sparse(4, [(0, 1.0), (3, -2.0)]),),\n",
        "        (Vectors.dense([4.0, 5.0, 0.0, 3.0]),),\n",
        "        (Vectors.dense([6.0, 7.0, 0.0, 8.0]),),\n",
        "        (Vectors.sparse(4, [(0, 9.0), (3, 1.0)]),)]\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nxz9RoIDdm2e",
        "outputId": "e86e162d-0357-4c08-8d64-b7e3c3e1ba1f"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(SparseVector(4, {0: 1.0, 3: -2.0}),),\n",
              " (DenseVector([4.0, 5.0, 0.0, 3.0]),),\n",
              " (DenseVector([6.0, 7.0, 0.0, 8.0]),),\n",
              " (SparseVector(4, {0: 9.0, 3: 1.0}),)]"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = spark.createDataFrame(data, [\"features\"])\n",
        "df.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7pNt3Xogdmut",
        "outputId": "2bf49b58-58d1-4c1d-d280-4091377ed5db"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+\n",
            "|            features|\n",
            "+--------------------+\n",
            "|(4,[0,3],[1.0,-2.0])|\n",
            "|   [4.0,5.0,0.0,3.0]|\n",
            "|   [6.0,7.0,0.0,8.0]|\n",
            "| (4,[0,3],[9.0,1.0])|\n",
            "+--------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.printSchema()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U5HFmxmPdw-Z",
        "outputId": "22746c2e-0696-4578-9b25-3c551e0dae26"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- features: vector (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Correlation**\n",
        "Calculating the correlation between two series of data is a common operation in Statistics. In spark.ml we provide the flexibility to calculate pairwise correlations among many series. The supported correlation methods are currently **Pearson’s and Spearman’s correlation**.\n",
        "\n"
      ],
      "metadata": {
        "id": "rx4wAAOVecnc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "r1 = Correlation.corr(df, \"features\").head()\n",
        "\n",
        "\n",
        "print(\"Pearson correlation matrix:\\n\" + str(r1[0]))\n",
        "\n",
        "r2 = Correlation.corr(df, \"features\", \"spearman\").head()\n",
        "\n",
        "\n",
        "print(\"Spearman correlation matrix:\\n\" + str(r2[0]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HM79ESA-dmkQ",
        "outputId": "2594ca8a-c2b4-45dc-e90b-40bb61e8d2c2"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pearson correlation matrix:\n",
            "DenseMatrix([[1.        , 0.05564149,        nan, 0.40047142],\n",
            "             [0.05564149, 1.        ,        nan, 0.91359586],\n",
            "             [       nan,        nan, 1.        ,        nan],\n",
            "             [0.40047142, 0.91359586,        nan, 1.        ]])\n",
            "Spearman correlation matrix:\n",
            "DenseMatrix([[1.        , 0.10540926,        nan, 0.4       ],\n",
            "             [0.10540926, 1.        ,        nan, 0.9486833 ],\n",
            "             [       nan,        nan, 1.        ,        nan],\n",
            "             [0.4       , 0.9486833 ,        nan, 1.        ]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Hypothesis testing**\n",
        "\n",
        "Hypothesis testing is a powerful tool in statistics to determine whether a result is statistically significant, whether this result occurred by chance or not. spark.ml currently supports Pearson’s **Chi-squared ( χ2\n",
        ")** tests for independence.\n",
        "\n",
        "#**ChiSquareTest**\n",
        "\n",
        "ChiSquareTest conducts **Pearson’s independence test** for every feature against the label. For each feature, the (feature, label) pairs are converted into a contingency matrix for which the **Chi-squared statistic** is computed. All label and feature values must be categorical."
      ],
      "metadata": {
        "id": "DNZaXLMDfnCE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "spark = SparkSession.builder.appName('Hyptest').getOrCreate()"
      ],
      "metadata": {
        "id": "O904fF0ZdM4A"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.linalg import Vectors\n",
        "from pyspark.ml.stat import ChiSquareTest\n",
        "\n",
        "data = [(0.0, Vectors.dense(0.5, 10.0)),\n",
        "        (0.0, Vectors.dense(1.5, 20.0)),\n",
        "        (1.0, Vectors.dense(1.5, 30.0)),\n",
        "        (0.0, Vectors.dense(3.5, 30.0)),\n",
        "        (0.0, Vectors.dense(3.5, 40.0)),\n",
        "        (1.0, Vectors.dense(3.5, 40.0))]\n",
        "df = spark.createDataFrame(data, [\"label\", \"features\"])\n",
        "\n",
        "r = ChiSquareTest.test(df, \"features\", \"label\").head()\n",
        "\n",
        "\n",
        "\n",
        "print(\"pValues: \" + str(r.pValues))\n",
        "print(\"degreesOfFreedom: \" + str(r.degreesOfFreedom))\n",
        "print(\"statistics: \" + str(r.statistics))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rW3SKtGRfiRe",
        "outputId": "82f79c8b-055f-47f9-e6e9-6a8d0377e680"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "pValues: [0.6872892787909721,0.6822703303362126]\n",
            "degreesOfFreedom: [2, 3]\n",
            "statistics: [0.75,1.5]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Summarizer**\n",
        "\n",
        "We provide vector column summary statistics for Dataframe through Summarizer. Available metrics are the column-wise **max, min, mean, sum, variance, std, and number of nonzeros**, as well as the total count.\n",
        "\n"
      ],
      "metadata": {
        "id": "QRzbNgjmgP2z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark import SparkContext, SparkConf\n",
        "\n",
        "# Create a SparkContext\n",
        "conf = SparkConf().setAppName(\"Summarizer\").setMaster(\"local\")\n",
        "sc = SparkContext.getOrCreate(conf=conf)"
      ],
      "metadata": {
        "id": "uUGbCEC8gdFJ"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.stat import Summarizer\n",
        "from pyspark.sql import Row\n",
        "from pyspark.ml.linalg import Vectors\n",
        "\n",
        "df = sc.parallelize([Row(weight=1.0, features=Vectors.dense(1.0, 1.0, 1.0)),\n",
        "                     Row(weight=0.0, features=Vectors.dense(1.0, 2.0, 3.0))]).toDF()\n",
        "\n",
        "# create summarizer for multiple metrics \"mean\" and \"count\"\n",
        "summarizer = Summarizer.metrics(\"mean\", \"count\")\n",
        "\n",
        "# compute statistics for multiple metrics with weight\n",
        "df.select(summarizer.summary(df.features, df.weight)).show(truncate=False)\n",
        "\n",
        "# compute statistics for multiple metrics without weight\n",
        "df.select(summarizer.summary(df.features)).show(truncate=False)\n",
        "\n",
        "# compute statistics for single metric \"mean\" with weight\n",
        "df.select(Summarizer.mean(df.features, df.weight)).show(truncate=False)\n",
        "\n",
        "# compute statistics for single metric \"mean\" without weight\n",
        "df.select(Summarizer.mean(df.features)).show(truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W7RFuD0ufi_z",
        "outputId": "f3cd4b9a-4df1-48eb-f530-63c2cd0746fd"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------------------------------+\n",
            "|aggregate_metrics(features, weight)|\n",
            "+-----------------------------------+\n",
            "|{[1.0,1.0,1.0], 1}                 |\n",
            "+-----------------------------------+\n",
            "\n",
            "+--------------------------------+\n",
            "|aggregate_metrics(features, 1.0)|\n",
            "+--------------------------------+\n",
            "|{[1.0,1.5,2.0], 2}              |\n",
            "+--------------------------------+\n",
            "\n",
            "+--------------+\n",
            "|mean(features)|\n",
            "+--------------+\n",
            "|[1.0,1.0,1.0] |\n",
            "+--------------+\n",
            "\n",
            "+--------------+\n",
            "|mean(features)|\n",
            "+--------------+\n",
            "|[1.0,1.5,2.0] |\n",
            "+--------------+\n",
            "\n"
          ]
        }
      ]
    }
  ]
}